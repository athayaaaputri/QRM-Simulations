{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Mixture Models and the EM Algorithm\n",
    "\n",
    "## Quantitative Risk Management - Week 2\n",
    "\n",
    "This notebook implements:\n",
    "1. Two-point normal mixture distributions\n",
    "2. Expectation-Maximization (EM) algorithm for parameter estimation\n",
    "3. Monte Carlo simulation for VaR and ES estimation\n",
    "4. Comparison with standard normal distributions\n",
    "\n",
    "### Theory Overview\n",
    "\n",
    "A two-point normal mixture model assumes losses follow:\n",
    "$$F(x) = \\lambda F_1(x; \\mu_1, \\sigma_1^2) + (1-\\lambda) F_2(x; \\mu_2, \\sigma_2^2)$$\n",
    "\n",
    "where:\n",
    "- $\\lambda$ = probability of \"good\" regime\n",
    "- $F_1$ = Normal distribution in good regime (low volatility)\n",
    "- $F_2$ = Normal distribution in bad regime (high volatility)\n",
    "\n",
    "This captures **leptokurtosis** (fat tails) observed in financial returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulating from a Normal Mixture Model\n",
    "\n",
    "### Pedagogical Parameters\n",
    "We use clear parameter values to understand the model:\n",
    "\n",
    "**Good Regime (70% probability):**\n",
    "- Mean: μ₁ = 0.5%\n",
    "- Std Dev: σ₁ = 1.5%\n",
    "\n",
    "**Bad Regime (30% probability):**\n",
    "- Mean: μ₂ = -1.0%\n",
    "- Std Dev: σ₂ = 4.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalMixtureModel:\n",
    "    \"\"\"\n",
    "    Two-point Normal Mixture Model for losses/returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lambda_: float\n",
    "        Probability of regime 1 (0 < lambda < 1)\n",
    "    mu1, mu2: float\n",
    "        Means of the two normal distributions\n",
    "    sigma1, sigma2: float\n",
    "        Standard deviations of the two distributions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_, mu1, sigma1, mu2, sigma2):\n",
    "        self.lambda_ = lambda_\n",
    "        self.mu1 = mu1\n",
    "        self.sigma1 = sigma1\n",
    "        self.mu2 = mu2\n",
    "        self.sigma2 = sigma2\n",
    "        \n",
    "    def simulate(self, n_samples, random_state=None):\n",
    "        \"\"\"\n",
    "        Simulate from the mixture model.\n",
    "        \n",
    "        Algorithm (from lecture slide 22):\n",
    "        1. Simulate regime indicators B_i ~ Bernoulli(lambda)\n",
    "        2. Simulate standard normals Z_i ~ N(0,1)\n",
    "        3. If B_i = 1: X_i = mu1 + sigma1 * Z_i\n",
    "           If B_i = 0: X_i = mu2 + sigma2 * Z_i\n",
    "        \"\"\"\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "            \n",
    "        # Step 1: Simulate regime indicators\n",
    "        regimes = np.random.binomial(1, self.lambda_, n_samples)\n",
    "        \n",
    "        # Step 2: Simulate standard normals\n",
    "        z = np.random.standard_normal(n_samples)\n",
    "        \n",
    "        # Step 3: Generate mixture samples\n",
    "        samples = np.where(regimes == 1,\n",
    "                          self.mu1 + self.sigma1 * z,\n",
    "                          self.mu2 + self.sigma2 * z)\n",
    "        \n",
    "        return samples, regimes\n",
    "    \n",
    "    def pdf(self, x):\n",
    "        \"\"\"Probability density function of the mixture.\"\"\"\n",
    "        pdf1 = stats.norm.pdf(x, self.mu1, self.sigma1)\n",
    "        pdf2 = stats.norm.pdf(x, self.mu2, self.sigma2)\n",
    "        return self.lambda_ * pdf1 + (1 - self.lambda_) * pdf2\n",
    "    \n",
    "    def cdf(self, x):\n",
    "        \"\"\"Cumulative distribution function of the mixture.\"\"\"\n",
    "        cdf1 = stats.norm.cdf(x, self.mu1, self.sigma1)\n",
    "        cdf2 = stats.norm.cdf(x, self.mu2, self.sigma2)\n",
    "        return self.lambda_ * cdf1 + (1 - self.lambda_) * cdf2\n",
    "    \n",
    "    def theoretical_moments(self):\n",
    "        \"\"\"\n",
    "        Calculate theoretical moments (from slide 19).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict with mean, variance, skewness, kurtosis\n",
    "        \"\"\"\n",
    "        # Mean\n",
    "        mean = self.lambda_ * self.mu1 + (1 - self.lambda_) * self.mu2\n",
    "        \n",
    "        # Variance (slide 19 formula)\n",
    "        var = (self.lambda_ * self.sigma1**2 + (1 - self.lambda_) * self.sigma2**2 +\n",
    "               self.lambda_ * (1 - self.lambda_) * (self.mu1 - self.mu2)**2)\n",
    "        \n",
    "        std = np.sqrt(var)\n",
    "        \n",
    "        # For skewness and kurtosis, we need higher moments\n",
    "        # Third central moment\n",
    "        m1_3 = (self.mu1 - mean)**3 + 3*(self.mu1 - mean)*self.sigma1**2\n",
    "        m2_3 = (self.mu2 - mean)**3 + 3*(self.mu2 - mean)*self.sigma2**2\n",
    "        mu3 = self.lambda_ * m1_3 + (1 - self.lambda_) * m2_3\n",
    "        \n",
    "        # Fourth central moment\n",
    "        m1_4 = (self.mu1 - mean)**4 + 6*(self.mu1 - mean)**2*self.sigma1**2 + 3*self.sigma1**4\n",
    "        m2_4 = (self.mu2 - mean)**4 + 6*(self.mu2 - mean)**2*self.sigma2**2 + 3*self.sigma2**4\n",
    "        mu4 = self.lambda_ * m1_4 + (1 - self.lambda_) * m2_4\n",
    "        \n",
    "        skewness = mu3 / std**3\n",
    "        kurtosis = mu4 / var**2\n",
    "        excess_kurtosis = kurtosis - 3\n",
    "        \n",
    "        return {\n",
    "            'mean': mean,\n",
    "            'variance': var,\n",
    "            'std': std,\n",
    "            'skewness': skewness,\n",
    "            'kurtosis': kurtosis,\n",
    "            'excess_kurtosis': excess_kurtosis\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define true parameters (pedagogical values)\n",
    "TRUE_PARAMS = {\n",
    "    'lambda_': 0.7,\n",
    "    'mu1': 0.5,      # Good regime: small positive mean\n",
    "    'sigma1': 1.5,   # Good regime: low volatility\n",
    "    'mu2': -1.0,     # Bad regime: negative mean (losses)\n",
    "    'sigma2': 4.0    # Bad regime: high volatility\n",
    "}\n",
    "\n",
    "# Create model instance\n",
    "true_model = NormalMixtureModel(**TRUE_PARAMS)\n",
    "\n",
    "# Display theoretical properties\n",
    "print(\"=\"*60)\n",
    "print(\"TRUE MODEL PARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"λ (good regime prob):  {TRUE_PARAMS['lambda_']:.3f}\")\n",
    "print(f\"\\nGood Regime (λ = {TRUE_PARAMS['lambda_']:.1%}):\")\n",
    "print(f\"  μ₁ = {TRUE_PARAMS['mu1']:.2f}%\")\n",
    "print(f\"  σ₁ = {TRUE_PARAMS['sigma1']:.2f}%\")\n",
    "print(f\"\\nBad Regime (λ = {1-TRUE_PARAMS['lambda_']:.1%}):\")\n",
    "print(f\"  μ₂ = {TRUE_PARAMS['mu2']:.2f}%\")\n",
    "print(f\"  σ₂ = {TRUE_PARAMS['sigma2']:.2f}%\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THEORETICAL MOMENTS\")\n",
    "print(\"=\"*60)\n",
    "moments = true_model.theoretical_moments()\n",
    "for key, value in moments.items():\n",
    "    print(f\"{key:20s}: {value:10.4f}\")\n",
    "print(\"\\nNote: Excess kurtosis > 0 indicates fat tails (leptokurtosis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monte Carlo Simulation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate large sample for analysis\n",
    "N_SAMPLES = 10000\n",
    "samples, regimes = true_model.simulate(N_SAMPLES, random_state=42)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Histogram with theoretical density\n",
    "ax = axes[0, 0]\n",
    "ax.hist(samples, bins=80, density=True, alpha=0.7, \n",
    "        color='steelblue', edgecolor='black', label='Simulated data')\n",
    "x_range = np.linspace(samples.min(), samples.max(), 1000)\n",
    "ax.plot(x_range, true_model.pdf(x_range), 'r-', lw=2.5, \n",
    "        label='True mixture PDF', alpha=0.8)\n",
    "\n",
    "# Overlay component distributions\n",
    "pdf1 = stats.norm.pdf(x_range, TRUE_PARAMS['mu1'], TRUE_PARAMS['sigma1'])\n",
    "pdf2 = stats.norm.pdf(x_range, TRUE_PARAMS['mu2'], TRUE_PARAMS['sigma2'])\n",
    "ax.plot(x_range, TRUE_PARAMS['lambda_'] * pdf1, 'g--', \n",
    "        lw=1.5, label=f'Good regime (λ={TRUE_PARAMS[\"lambda_\"]:.1%})', alpha=0.6)\n",
    "ax.plot(x_range, (1-TRUE_PARAMS['lambda_']) * pdf2, 'm--', \n",
    "        lw=1.5, label=f'Bad regime (1-λ={1-TRUE_PARAMS[\"lambda_\"]:.1%})', alpha=0.6)\n",
    "\n",
    "ax.set_xlabel('Returns (%)', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Normal Mixture Distribution\\n(Entire Distribution)', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Zoomed-in right tail\n",
    "ax = axes[0, 1]\n",
    "tail_threshold = 3.0\n",
    "tail_samples = samples[samples > tail_threshold]\n",
    "ax.hist(tail_samples, bins=40, density=True, alpha=0.7,\n",
    "        color='indianred', edgecolor='black', label='Simulated data (tail)')\n",
    "x_tail = np.linspace(tail_threshold, samples.max(), 500)\n",
    "ax.plot(x_tail, true_model.pdf(x_tail), 'r-', lw=2.5, label='True PDF', alpha=0.8)\n",
    "\n",
    "# Compare with normal approximation\n",
    "normal_pdf = stats.norm.pdf(x_tail, samples.mean(), samples.std())\n",
    "ax.plot(x_tail, normal_pdf, 'b--', lw=2, label='Normal approx.', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Returns (%)', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title(f'Right Tail (x > {tail_threshold}%)\\nMixture vs Normal', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-Q plot vs Normal (from slide 12)\n",
    "ax = axes[1, 0]\n",
    "stats.probplot(samples, dist=\"norm\", plot=ax)\n",
    "ax.set_title('Q-Q Plot vs Standard Normal\\n(Checking for fat tails)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Regime classification\n",
    "ax = axes[1, 1]\n",
    "regime1_samples = samples[regimes == 1]\n",
    "regime2_samples = samples[regimes == 0]\n",
    "ax.hist(regime1_samples, bins=60, density=True, alpha=0.6, \n",
    "        color='green', label=f'Good regime ({len(regime1_samples)} obs)', edgecolor='black')\n",
    "ax.hist(regime2_samples, bins=60, density=True, alpha=0.6,\n",
    "        color='red', label=f'Bad regime ({len(regime2_samples)} obs)', edgecolor='black')\n",
    "ax.set_xlabel('Returns (%)', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Data by True Regime\\n(Hidden in practice)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mixture_distribution_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compare sample vs theoretical moments\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE vs THEORETICAL MOMENTS (N=10,000)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Moment':<20} {'Sample':>12} {'Theoretical':>12} {'Difference':>12}\")\n",
    "print(\"-\"*60)\n",
    "sample_mean = samples.mean()\n",
    "sample_std = samples.std()\n",
    "sample_skew = stats.skew(samples)\n",
    "sample_kurt = stats.kurtosis(samples, fisher=False)  # Pearson kurtosis\n",
    "\n",
    "print(f\"{'Mean':<20} {sample_mean:>12.4f} {moments['mean']:>12.4f} {sample_mean - moments['mean']:>12.4f}\")\n",
    "print(f\"{'Std Dev':<20} {sample_std:>12.4f} {moments['std']:>12.4f} {sample_std - moments['std']:>12.4f}\")\n",
    "print(f\"{'Skewness':<20} {sample_skew:>12.4f} {moments['skewness']:>12.4f} {sample_skew - moments['skewness']:>12.4f}\")\n",
    "print(f\"{'Kurtosis':<20} {sample_kurt:>12.4f} {moments['kurtosis']:>12.4f} {sample_kurt - moments['kurtosis']:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "### Algorithm Overview (from slides 26-31)\n",
    "\n",
    "The EM algorithm solves the MLE problem when regimes are unobserved:\n",
    "\n",
    "**Challenge:** Log-likelihood has a sum inside the logarithm:\n",
    "$$\\log L = \\sum_{i=1}^n \\log[\\lambda f_1(x_i) + (1-\\lambda)f_2(x_i)]$$\n",
    "\n",
    "**Solution:** Iterate between:\n",
    "- **E-step:** Calculate regime probabilities given current parameters\n",
    "- **M-step:** Update parameters given regime probabilities\n",
    "\n",
    "### E-Step (Slide 30)\n",
    "Calculate conditional probability that observation $i$ came from regime 1:\n",
    "$$p_i^{(t)} = \\frac{\\lambda^{(t)} f_1(x_i; \\mu_1^{(t)}, \\sigma_1^{(t)})}{\\lambda^{(t)} f_1(x_i; \\mu_1^{(t)}, \\sigma_1^{(t)}) + (1-\\lambda^{(t)}) f_2(x_i; \\mu_2^{(t)}, \\sigma_2^{(t)})}$$\n",
    "\n",
    "### M-Step (Slide 31)\n",
    "Update parameters using weighted MLE:\n",
    "$$\\lambda^{(t+1)} = \\frac{1}{n}\\sum_{i=1}^n p_i^{(t)}$$\n",
    "$$\\mu_1^{(t+1)} = \\frac{\\sum_{i=1}^n x_i p_i^{(t)}}{\\sum_{i=1}^n p_i^{(t)}}$$\n",
    "$$(\\sigma_1^{(t+1)})^2 = \\frac{\\sum_{i=1}^n (x_i - \\mu_1^{(t+1)})^2 p_i^{(t)}}{\\sum_{i=1}^n p_i^{(t)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAlgorithm:\n",
    "    \"\"\"\n",
    "    EM Algorithm for estimating Normal Mixture Model parameters.\n",
    "    \n",
    "    Implementation follows slides 26-31.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_iter=1000, tol=1e-6, verbose=True):\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.history = []\n",
    "        \n",
    "    def initialize_params(self, data, method='kmeans'):\n",
    "        \"\"\"\n",
    "        Initialize parameters. Good initialization is crucial!\n",
    "        \n",
    "        Methods:\n",
    "        - 'random': Random initialization\n",
    "        - 'kmeans': Use k-means clustering (more stable)\n",
    "        \"\"\"\n",
    "        if method == 'random':\n",
    "            lambda_ = np.random.uniform(0.3, 0.7)\n",
    "            mu1 = np.random.normal(data.mean(), data.std())\n",
    "            mu2 = np.random.normal(data.mean(), data.std())\n",
    "            sigma1 = np.random.uniform(data.std()/2, data.std())\n",
    "            sigma2 = np.random.uniform(data.std(), data.std()*2)\n",
    "        elif method == 'kmeans':\n",
    "            from sklearn.cluster import KMeans\n",
    "            kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(data.reshape(-1, 1))\n",
    "            \n",
    "            cluster1 = data[labels == 0]\n",
    "            cluster2 = data[labels == 1]\n",
    "            \n",
    "            lambda_ = len(cluster1) / len(data)\n",
    "            mu1 = cluster1.mean()\n",
    "            mu2 = cluster2.mean()\n",
    "            sigma1 = cluster1.std()\n",
    "            sigma2 = cluster2.std()\n",
    "        \n",
    "        return lambda_, mu1, sigma1, mu2, sigma2\n",
    "    \n",
    "    def e_step(self, data, lambda_, mu1, sigma1, mu2, sigma2):\n",
    "        \"\"\"\n",
    "        E-step: Calculate regime probabilities (slide 30).\n",
    "        \n",
    "        Returns p_i = P(regime 1 | x_i, params)\n",
    "        \"\"\"\n",
    "        # Calculate densities for each component\n",
    "        pdf1 = stats.norm.pdf(data, mu1, sigma1)\n",
    "        pdf2 = stats.norm.pdf(data, mu2, sigma2)\n",
    "        \n",
    "        # Calculate conditional probabilities (Bayes' rule)\n",
    "        numerator = lambda_ * pdf1\n",
    "        denominator = lambda_ * pdf1 + (1 - lambda_) * pdf2\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        p_regime1 = np.divide(numerator, denominator, \n",
    "                             out=np.ones_like(numerator)*0.5,\n",
    "                             where=denominator!=0)\n",
    "        \n",
    "        return p_regime1\n",
    "    \n",
    "    def m_step(self, data, p_regime1):\n",
    "        \"\"\"\n",
    "        M-step: Update parameters (slide 31).\n",
    "        \n",
    "        Uses weighted MLE formulas.\n",
    "        \"\"\"\n",
    "        n = len(data)\n",
    "        p_regime2 = 1 - p_regime1\n",
    "        \n",
    "        # Update lambda\n",
    "        lambda_new = p_regime1.sum() / n\n",
    "        \n",
    "        # Update mu1, mu2\n",
    "        mu1_new = (data * p_regime1).sum() / p_regime1.sum()\n",
    "        mu2_new = (data * p_regime2).sum() / p_regime2.sum()\n",
    "        \n",
    "        # Update sigma1, sigma2\n",
    "        sigma1_new = np.sqrt(((data - mu1_new)**2 * p_regime1).sum() / p_regime1.sum())\n",
    "        sigma2_new = np.sqrt(((data - mu2_new)**2 * p_regime2).sum() / p_regime2.sum())\n",
    "        \n",
    "        return lambda_new, mu1_new, sigma1_new, mu2_new, sigma2_new\n",
    "    \n",
    "    def log_likelihood(self, data, lambda_, mu1, sigma1, mu2, sigma2):\n",
    "        \"\"\"\n",
    "        Calculate log-likelihood (slide 31).\n",
    "        \n",
    "        log L = sum_i log[lambda * f1(x_i) + (1-lambda) * f2(x_i)]\n",
    "        \"\"\"\n",
    "        pdf1 = stats.norm.pdf(data, mu1, sigma1)\n",
    "        pdf2 = stats.norm.pdf(data, mu2, sigma2)\n",
    "        mixture_pdf = lambda_ * pdf1 + (1 - lambda_) * pdf2\n",
    "        \n",
    "        # Avoid log(0)\n",
    "        mixture_pdf = np.maximum(mixture_pdf, 1e-300)\n",
    "        \n",
    "        return np.sum(np.log(mixture_pdf))\n",
    "    \n",
    "    def fit(self, data, init_method='kmeans', n_restarts=5):\n",
    "        \"\"\"\n",
    "        Fit the model using EM algorithm with multiple random restarts.\n",
    "        \n",
    "        Multiple restarts help avoid local maxima (slide 33).\n",
    "        \"\"\"\n",
    "        best_loglik = -np.inf\n",
    "        best_params = None\n",
    "        best_history = None\n",
    "        \n",
    "        for restart in range(n_restarts):\n",
    "            if self.verbose:\n",
    "                print(f\"\\nRestart {restart + 1}/{n_restarts}\")\n",
    "                print(\"-\" * 50)\n",
    "            \n",
    "            # Initialize\n",
    "            lambda_, mu1, sigma1, mu2, sigma2 = self.initialize_params(data, init_method)\n",
    "            \n",
    "            history = []\n",
    "            loglik = self.log_likelihood(data, lambda_, mu1, sigma1, mu2, sigma2)\n",
    "            history.append({\n",
    "                'iter': 0,\n",
    "                'loglik': loglik,\n",
    "                'lambda': lambda_,\n",
    "                'mu1': mu1, 'sigma1': sigma1,\n",
    "                'mu2': mu2, 'sigma2': sigma2\n",
    "            })\n",
    "            \n",
    "            # EM iterations\n",
    "            for iteration in range(1, self.max_iter + 1):\n",
    "                # E-step\n",
    "                p_regime1 = self.e_step(data, lambda_, mu1, sigma1, mu2, sigma2)\n",
    "                \n",
    "                # M-step\n",
    "                lambda_, mu1, sigma1, mu2, sigma2 = self.m_step(data, p_regime1)\n",
    "                \n",
    "                # Calculate log-likelihood\n",
    "                loglik_new = self.log_likelihood(data, lambda_, mu1, sigma1, mu2, sigma2)\n",
    "                \n",
    "                history.append({\n",
    "                    'iter': iteration,\n",
    "                    'loglik': loglik_new,\n",
    "                    'lambda': lambda_,\n",
    "                    'mu1': mu1, 'sigma1': sigma1,\n",
    "                    'mu2': mu2, 'sigma2': sigma2\n",
    "                })\n",
    "                \n",
    "                # Check convergence (slide 31)\n",
    "                if abs(loglik_new - loglik) < self.tol:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Converged at iteration {iteration}\")\n",
    "                        print(f\"Final log-likelihood: {loglik_new:.4f}\")\n",
    "                    break\n",
    "                    \n",
    "                loglik = loglik_new\n",
    "                \n",
    "                if self.verbose and iteration % 20 == 0:\n",
    "                    print(f\"Iter {iteration:3d}: log-lik = {loglik_new:.4f}\")\n",
    "            \n",
    "            # Keep best result\n",
    "            if loglik_new > best_loglik:\n",
    "                best_loglik = loglik_new\n",
    "                best_params = (lambda_, mu1, sigma1, mu2, sigma2)\n",
    "                best_history = history\n",
    "        \n",
    "        self.history = best_history\n",
    "        return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model to Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate estimation sample (smaller than before)\n",
    "N_ESTIMATION = 2000\n",
    "estimation_data, _ = true_model.simulate(N_ESTIMATION, random_state=123)\n",
    "\n",
    "# Run EM algorithm\n",
    "print(\"=\"*60)\n",
    "print(\"FITTING MODEL USING EM ALGORITHM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Sample size: {N_ESTIMATION:,}\")\n",
    "print(f\"Number of restarts: 5\")\n",
    "\n",
    "em = EMAlgorithm(max_iter=200, tol=1e-6, verbose=True)\n",
    "lambda_hat, mu1_hat, sigma1_hat, mu2_hat, sigma2_hat = em.fit(estimation_data, \n",
    "                                                                init_method='kmeans',\n",
    "                                                                n_restarts=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare estimated vs true parameters\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PARAMETER ESTIMATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Parameter':<15} {'True':>12} {'Estimated':>12} {'Error':>12} {'Error %':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "params_comparison = [\n",
    "    ('λ', TRUE_PARAMS['lambda_'], lambda_hat),\n",
    "    ('μ₁', TRUE_PARAMS['mu1'], mu1_hat),\n",
    "    ('σ₁', TRUE_PARAMS['sigma1'], sigma1_hat),\n",
    "    ('μ₂', TRUE_PARAMS['mu2'], mu2_hat),\n",
    "    ('σ₂', TRUE_PARAMS['sigma2'], sigma2_hat),\n",
    "]\n",
    "\n",
    "for name, true_val, est_val in params_comparison:\n",
    "    error = est_val - true_val\n",
    "    error_pct = (error / true_val * 100) if true_val != 0 else 0\n",
    "    print(f\"{name:<15} {true_val:>12.4f} {est_val:>12.4f} {error:>12.4f} {error_pct:>11.2f}%\")\n",
    "\n",
    "# Note on label switching\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTE: If parameters appear swapped, this is 'label switching' - \")\n",
    "print(\"      the EM algorithm found the same model but swapped regime labels.\")\n",
    "print(\"      This is normal and doesn't affect the mixture distribution.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize EM convergence\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "history_df = pd.DataFrame(em.history)\n",
    "\n",
    "# Log-likelihood convergence\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history_df['iter'], history_df['loglik'], 'b-', lw=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Log-Likelihood')\n",
    "ax.set_title('EM Convergence\\n(Slide 31: L(t+1) ≥ L(t))', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Lambda convergence\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history_df['iter'], history_df['lambda'], 'g-', lw=2, label='Estimated')\n",
    "ax.axhline(TRUE_PARAMS['lambda_'], color='r', linestyle='--', lw=2, label='True value')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('λ')\n",
    "ax.set_title('λ Parameter Convergence', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Mu convergence\n",
    "ax = axes[0, 2]\n",
    "ax.plot(history_df['iter'], history_df['mu1'], 'b-', lw=2, label='μ₁ (estimated)')\n",
    "ax.plot(history_df['iter'], history_df['mu2'], 'm-', lw=2, label='μ₂ (estimated)')\n",
    "ax.axhline(TRUE_PARAMS['mu1'], color='b', linestyle='--', lw=1.5, label='μ₁ (true)')\n",
    "ax.axhline(TRUE_PARAMS['mu2'], color='m', linestyle='--', lw=1.5, label='μ₂ (true)')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Mean')\n",
    "ax.set_title('Mean Parameters Convergence', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Sigma convergence\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history_df['iter'], history_df['sigma1'], 'b-', lw=2, label='σ₁ (estimated)')\n",
    "ax.plot(history_df['iter'], history_df['sigma2'], 'm-', lw=2, label='σ₂ (estimated)')\n",
    "ax.axhline(TRUE_PARAMS['sigma1'], color='b', linestyle='--', lw=1.5, label='σ₁ (true)')\n",
    "ax.axhline(TRUE_PARAMS['sigma2'], color='m', linestyle='--', lw=1.5, label='σ₂ (true)')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Std Dev')\n",
    "ax.set_title('Std Dev Parameters Convergence', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Fitted vs True density\n",
    "ax = axes[1, 1]\n",
    "ax.hist(estimation_data, bins=60, density=True, alpha=0.5, \n",
    "        color='gray', label='Data')\n",
    "x_range = np.linspace(estimation_data.min(), estimation_data.max(), 500)\n",
    "\n",
    "# True density\n",
    "true_pdf = true_model.pdf(x_range)\n",
    "ax.plot(x_range, true_pdf, 'r-', lw=2.5, label='True mixture', alpha=0.8)\n",
    "\n",
    "# Estimated density\n",
    "estimated_model = NormalMixtureModel(lambda_hat, mu1_hat, sigma1_hat, mu2_hat, sigma2_hat)\n",
    "est_pdf = estimated_model.pdf(x_range)\n",
    "ax.plot(x_range, est_pdf, 'b--', lw=2.5, label='Estimated mixture', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Returns (%)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Fitted vs True Distribution', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter trajectory in 2D space\n",
    "ax = axes[1, 2]\n",
    "ax.plot(history_df['mu1'], history_df['sigma1'], 'bo-', markersize=3, lw=1.5, \n",
    "        alpha=0.6, label='(μ₁, σ₁) path')\n",
    "ax.plot(history_df['mu2'], history_df['sigma2'], 'mo-', markersize=3, lw=1.5,\n",
    "        alpha=0.6, label='(μ₂, σ₂) path')\n",
    "ax.plot(TRUE_PARAMS['mu1'], TRUE_PARAMS['sigma1'], 'b*', markersize=15, \n",
    "        label='True (μ₁, σ₁)')\n",
    "ax.plot(TRUE_PARAMS['mu2'], TRUE_PARAMS['sigma2'], 'm*', markersize=15,\n",
    "        label='True (μ₂, σ₂)')\n",
    "ax.set_xlabel('Mean (μ)')\n",
    "ax.set_ylabel('Std Dev (σ)')\n",
    "ax.set_title('Parameter Space Trajectory', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('em_convergence_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monte Carlo VaR and ES Estimation\n",
    "\n",
    "### Methodology (from slides 20-22)\n",
    "\n",
    "1. **Simulate large sample** from the fitted mixture model (m = 100,000)\n",
    "2. **Calculate VaR** using empirical quantile (Historical Simulation on simulated data)\n",
    "3. **Calculate ES** as average of losses beyond VaR\n",
    "4. **Convergence analysis**: How does estimate improve with m?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_risk_measures(model, n_simulations, confidence_levels, random_state=None):\n",
    "    \"\"\"\n",
    "    Calculate VaR and ES using Monte Carlo simulation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: NormalMixtureModel\n",
    "        Fitted mixture model\n",
    "    n_simulations: int\n",
    "        Number of MC samples\n",
    "    confidence_levels: list\n",
    "        List of confidence levels (e.g., [0.95, 0.99, 0.995])\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with VaR and ES estimates\n",
    "    \"\"\"\n",
    "    # Simulate from model\n",
    "    simulated_losses, _ = model.simulate(n_simulations, random_state=random_state)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for alpha in confidence_levels:\n",
    "        # VaR: empirical quantile (Historical Simulation method)\n",
    "        var = np.percentile(simulated_losses, alpha * 100)\n",
    "        \n",
    "        # ES: average of losses exceeding VaR\n",
    "        tail_losses = simulated_losses[simulated_losses >= var]\n",
    "        es = tail_losses.mean() if len(tail_losses) > 0 else var\n",
    "        \n",
    "        results.append({\n",
    "            'Confidence': f\"{alpha:.1%}\",\n",
    "            'Alpha': alpha,\n",
    "            'VaR': var,\n",
    "            'ES': es,\n",
    "            'ES/VaR': es/var if var != 0 else np.nan,\n",
    "            'N_tail': len(tail_losses)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Calculate risk measures for true and estimated models\n",
    "CONFIDENCE_LEVELS = [0.90, 0.95, 0.975, 0.99, 0.995, 0.999]\n",
    "N_MC = 100000\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"MONTE CARLO RISK MEASURES (N = {N_MC:,} simulations)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# True model\n",
    "print(\"\\nTRUE MODEL:\")\n",
    "true_risk = monte_carlo_risk_measures(true_model, N_MC, CONFIDENCE_LEVELS, random_state=42)\n",
    "print(true_risk.to_string(index=False))\n",
    "\n",
    "# Estimated model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTIMATED MODEL:\")\n",
    "estimated_risk = monte_carlo_risk_measures(estimated_model, N_MC, CONFIDENCE_LEVELS, random_state=43)\n",
    "print(estimated_risk.to_string(index=False))\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTIMATION ERROR:\")\n",
    "print(\"=\"*80)\n",
    "comparison = pd.DataFrame({\n",
    "    'Confidence': true_risk['Confidence'],\n",
    "    'VaR_Error': estimated_risk['VaR'] - true_risk['VaR'],\n",
    "    'VaR_Error_%': ((estimated_risk['VaR'] - true_risk['VaR']) / true_risk['VaR'] * 100),\n",
    "    'ES_Error': estimated_risk['ES'] - true_risk['ES'],\n",
    "    'ES_Error_%': ((estimated_risk['ES'] - true_risk['ES']) / true_risk['ES'] * 100)\n",
    "})\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze convergence as function of sample size\n",
    "sample_sizes = [100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "target_alpha = 0.99\n",
    "n_replications = 50  # Repeat each sample size to get variability\n",
    "\n",
    "print(f\"\\nAnalyzing MC convergence for VaR{target_alpha:.0%}...\")\n",
    "print(f\"Running {n_replications} replications for each sample size...\")\n",
    "\n",
    "convergence_results = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    var_estimates = []\n",
    "    es_estimates = []\n",
    "    \n",
    "    for rep in range(n_replications):\n",
    "        result = monte_carlo_risk_measures(true_model, n, [target_alpha], \n",
    "                                           random_state=rep)\n",
    "        var_estimates.append(result['VaR'].iloc[0])\n",
    "        es_estimates.append(result['ES'].iloc[0])\n",
    "    \n",
    "    convergence_results.append({\n",
    "        'n': n,\n",
    "        'var_mean': np.mean(var_estimates),\n",
    "        'var_std': np.std(var_estimates),\n",
    "        'es_mean': np.mean(es_estimates),\n",
    "        'es_std': np.std(es_estimates)\n",
    "    })\n",
    "\n",
    "convergence_df = pd.DataFrame(convergence_results)\n",
    "\n",
    "# Plot convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Get \"true\" value from very large sample\n",
    "true_mc = monte_carlo_risk_measures(true_model, 500000, [target_alpha], random_state=999)\n",
    "true_var = true_mc['VaR'].iloc[0]\n",
    "true_es = true_mc['ES'].iloc[0]\n",
    "\n",
    "# VaR convergence\n",
    "ax = axes[0]\n",
    "ax.errorbar(convergence_df['n'], convergence_df['var_mean'], \n",
    "            yerr=1.96*convergence_df['var_std'], \n",
    "            fmt='bo-', capsize=5, capthick=2, label='MC estimates (95% CI)')\n",
    "ax.axhline(true_var, color='r', linestyle='--', lw=2, label=f'True VaR ≈ {true_var:.3f}')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Number of MC Simulations', fontsize=11)\n",
    "ax.set_ylabel(f'VaR{target_alpha:.0%}', fontsize=11)\n",
    "ax.set_title(f'VaR{target_alpha:.0%} Monte Carlo Convergence\\n({n_replications} replications per size)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ES convergence\n",
    "ax = axes[1]\n",
    "ax.errorbar(convergence_df['n'], convergence_df['es_mean'],\n",
    "            yerr=1.96*convergence_df['es_std'],\n",
    "            fmt='go-', capsize=5, capthick=2, label='MC estimates (95% CI)')\n",
    "ax.axhline(true_es, color='r', linestyle='--', lw=2, label=f'True ES ≈ {true_es:.3f}')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Number of MC Simulations', fontsize=11)\n",
    "ax.set_ylabel(f'ES{target_alpha:.0%}', fontsize=11)\n",
    "ax.set_title(f'ES{target_alpha:.0%} Monte Carlo Convergence\\n({n_replications} replications per size)',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mc_convergence_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConvergence Summary:\")\n",
    "print(convergence_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison with Normal Model\n",
    "\n",
    "### Why Mixture Models Matter\n",
    "\n",
    "Compare risk measures from:\n",
    "1. **Normal Mixture** (captures leptokurtosis)\n",
    "2. **Standard Normal** (traditional approach, underestimates tail risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit normal distribution to the same data\n",
    "normal_mean = estimation_data.mean()\n",
    "normal_std = estimation_data.std()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NORMAL vs MIXTURE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate risk measures for normal model\n",
    "normal_risk_measures = []\n",
    "for alpha in CONFIDENCE_LEVELS:\n",
    "    # VaR from normal: mean + z_alpha * std\n",
    "    z_alpha = stats.norm.ppf(alpha)\n",
    "    var_normal = normal_mean + z_alpha * normal_std\n",
    "    \n",
    "    # ES from normal: mean + phi(z_alpha)/((1-alpha)) * std\n",
    "    es_normal = normal_mean + (stats.norm.pdf(z_alpha) / (1 - alpha)) * normal_std\n",
    "    \n",
    "    normal_risk_measures.append({\n",
    "        'Confidence': f\"{alpha:.1%}\",\n",
    "        'VaR_Normal': var_normal,\n",
    "        'ES_Normal': es_normal\n",
    "    })\n",
    "\n",
    "normal_df = pd.DataFrame(normal_risk_measures)\n",
    "\n",
    "# Combine with mixture estimates\n",
    "comparison_df = pd.merge(\n",
    "    estimated_risk[['Confidence', 'VaR', 'ES']], \n",
    "    normal_df, \n",
    "    on='Confidence'\n",
    ")\n",
    "comparison_df['VaR_Underest_%'] = ((comparison_df['VaR_Normal'] - comparison_df['VaR']) / \n",
    "                                    comparison_df['VaR'] * 100)\n",
    "comparison_df['ES_Underest_%'] = ((comparison_df['ES_Normal'] - comparison_df['ES']) / \n",
    "                                   comparison_df['ES'] * 100)\n",
    "\n",
    "print(\"\\nRisk Measure Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nNegative % = Normal model UNDERESTIMATES tail risk\")\n",
    "print(\"(Normal ignores fat tails from mixture structure)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# VaR comparison\n",
    "ax = axes[0]\n",
    "x = np.arange(len(CONFIDENCE_LEVELS))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, comparison_df['VaR'], width, label='Mixture Model', \n",
    "       color='steelblue', edgecolor='black')\n",
    "ax.bar(x + width/2, comparison_df['VaR_Normal'], width, label='Normal Model',\n",
    "       color='lightcoral', edgecolor='black')\n",
    "ax.set_xlabel('Confidence Level', fontsize=11)\n",
    "ax.set_ylabel('VaR', fontsize=11)\n",
    "ax.set_title('VaR: Mixture vs Normal Model\\n(Normal underestimates tail risk)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Confidence'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ES comparison\n",
    "ax = axes[1]\n",
    "ax.bar(x - width/2, comparison_df['ES'], width, label='Mixture Model',\n",
    "       color='steelblue', edgecolor='black')\n",
    "ax.bar(x + width/2, comparison_df['ES_Normal'], width, label='Normal Model',\n",
    "       color='lightcoral', edgecolor='black')\n",
    "ax.set_xlabel('Confidence Level', fontsize=11)\n",
    "ax.set_ylabel('ES', fontsize=11)\n",
    "ax.set_title('ES: Mixture vs Normal Model\\n(Normal underestimates tail risk)',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Confidence'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mixture_vs_normal_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "### Normal Mixture Models\n",
    "- Capture **leptokurtosis** (fat tails) and **skewness** in financial returns\n",
    "- Simple two-regime interpretation: \"good times\" vs \"bad times\"\n",
    "- NOT the same as normal distribution (slide 17)\n",
    "\n",
    "### EM Algorithm\n",
    "- Solves MLE when regimes are **hidden/unobserved**\n",
    "- Iterates between E-step (guess regimes) and M-step (update parameters)\n",
    "- Guaranteed to improve likelihood: $L^{(t+1)} \\geq L^{(t)}$\n",
    "- Multiple random restarts help avoid local maxima\n",
    "\n",
    "### Monte Carlo Risk Estimation\n",
    "- VaR: Empirical quantile of simulated losses\n",
    "- ES: Average of tail losses beyond VaR\n",
    "- Convergence improves with $\\sqrt{n}$\n",
    "- For 99.9% quantiles, need 100,000+ simulations\n",
    "\n",
    "### Practical Implications\n",
    "- Normal models **systematically underestimate tail risk**\n",
    "- Underestimation worsens at extreme quantiles (99%+)\n",
    "- Critical for emerging markets with regime-switching volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "### Exercise 1: Sensitivity Analysis\n",
    "Modify the true parameters to create a more asymmetric mixture (e.g., λ=0.9, very different means). How does this affect:\n",
    "- Skewness?\n",
    "- EM convergence speed?\n",
    "- Difference between Normal and Mixture VaR?\n",
    "\n",
    "### Exercise 2: Three-Component Mixture\n",
    "Extend the code to handle three regimes (good/normal/bad). What challenges arise in:\n",
    "- Parameter initialization?\n",
    "- Convergence?\n",
    "- Label switching?\n",
    "\n",
    "### Exercise 3: Real Data Application\n",
    "Download emerging market equity returns (e.g., MSCI Emerging Markets index). \n",
    "- Fit both normal and mixture models\n",
    "- Compare VaR/ES estimates\n",
    "- Validate with actual crisis events\n",
    "\n",
    "### Exercise 4: Exponential Mixture (Slide 72 Problem 1)\n",
    "Implement the EM algorithm for exponential mixture distributions as described in the homework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
