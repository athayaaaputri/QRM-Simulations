{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heavy-Tailed Distributions and Risk Measure Comparison\n",
    "\n",
    "## Quantitative Risk Management - Week 2\n",
    "\n",
    "This notebook implements:\n",
    "1. Heavy-tailed distribution theory and simulation\n",
    "2. VaR vs ES: Properties and coherence\n",
    "3. Probability shifting under heavy tails\n",
    "4. Monte Carlo analysis of risk measure relationships\n",
    "\n",
    "### Theory Overview\n",
    "\n",
    "**Heavy-Tailed Distribution (Slide 49):**\n",
    "$$\\lim_{t\\to\\infty} \\frac{1-F(tx)}{1-F(t)} = x^{-\\alpha}$$\n",
    "\n",
    "where $\\alpha > 0$ is the **tail index**.\n",
    "\n",
    "**Key Implications:**\n",
    "- Lower $\\alpha$ → Heavier tails\n",
    "- Moments $E[X^k]$ exist only for $k < \\alpha$\n",
    "- Common in financial returns: $\\alpha \\in [3, 4]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize_scalar, brentq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Heavy-Tailed Distributions: Examples and Properties\n",
    "\n",
    "### Pedagogical Examples (Slide 51)\n",
    "\n",
    "We implement three distributions:\n",
    "\n",
    "1. **Pareto Distribution** (exactly heavy-tailed):\n",
    "   $$F(x) = 1 - \\left(\\frac{x_m}{x}\\right)^\\alpha, \\quad x \\geq x_m$$\n",
    "   \n",
    "2. **Student-t Distribution** (heavy-tailed, $\\alpha = \\nu$):\n",
    "   - Degrees of freedom $\\nu$ = tail index\n",
    "   - Popular for financial modeling\n",
    "   \n",
    "3. **Fréchet Distribution** (heavy-tailed, $\\alpha = 1/\\gamma$):\n",
    "   $$F(x) = \\exp\\left(-x^{-\\alpha}\\right), \\quad x > 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class HeavyTailedDistribution:\n",
    "    \"\"\"\n",
    "    Base class for heavy-tailed distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def simulate(self, n, random_state=None):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def pdf(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def cdf(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def var(self, p):\n",
    "        \"\"\"Value at Risk at confidence level p.\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def es(self, p):\n",
    "        \"\"\"Expected Shortfall at confidence level p.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ParetoDistribution(HeavyTailedDistribution):\n",
    "    \"\"\"\n",
    "    Pareto Distribution: F(x) = 1 - (x_m/x)^alpha for x >= x_m.\n",
    "    \n",
    "    This is the canonical heavy-tailed distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, x_m=1.0):\n",
    "        super().__init__(alpha)\n",
    "        self.x_m = x_m\n",
    "        \n",
    "    def simulate(self, n, random_state=None):\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        # Inverse transform sampling\n",
    "        u = np.random.uniform(0, 1, n)\n",
    "        return self.x_m / (1 - u) ** (1 / self.alpha)\n",
    "    \n",
    "    def pdf(self, x):\n",
    "        return np.where(x >= self.x_m,\n",
    "                       self.alpha * self.x_m**self.alpha / x**(self.alpha + 1),\n",
    "                       0)\n",
    "    \n",
    "    def cdf(self, x):\n",
    "        return np.where(x >= self.x_m,\n",
    "                       1 - (self.x_m / x)**self.alpha,\n",
    "                       0)\n",
    "    \n",
    "    def var(self, p):\n",
    "        \"\"\"Analytical VaR for Pareto.\"\"\"\n",
    "        return self.x_m / (1 - p) ** (1 / self.alpha)\n",
    "    \n",
    "    def es(self, p):\n",
    "        \"\"\"Analytical ES for Pareto (slide 55).\"\"\"\n",
    "        if self.alpha <= 1:\n",
    "            return np.inf\n",
    "        var_p = self.var(p)\n",
    "        return self.alpha / (self.alpha - 1) * var_p\n",
    "\n",
    "\n",
    "class StudentTDistribution(HeavyTailedDistribution):\n",
    "    \"\"\"\n",
    "    Standardized Student-t Distribution.\n",
    "    \n",
    "    Tail index alpha = nu (degrees of freedom).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nu, mu=0, sigma=1):\n",
    "        super().__init__(alpha=nu)\n",
    "        self.nu = nu\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def simulate(self, n, random_state=None):\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        return self.mu + self.sigma * stats.t.rvs(self.nu, size=n)\n",
    "    \n",
    "    def pdf(self, x):\n",
    "        return stats.t.pdf((x - self.mu) / self.sigma, self.nu) / self.sigma\n",
    "    \n",
    "    def cdf(self, x):\n",
    "        return stats.t.cdf((x - self.mu) / self.sigma, self.nu)\n",
    "    \n",
    "    def var(self, p):\n",
    "        return self.mu + self.sigma * stats.t.ppf(p, self.nu)\n",
    "    \n",
    "    def es(self, p):\n",
    "        \"\"\"ES for Student-t (numerical integration).\"\"\"\n",
    "        var_p = self.var(p)\n",
    "        # ES = E[X | X > VaR_p]\n",
    "        # For Student-t, we use the formula involving the PDF\n",
    "        z_p = stats.t.ppf(p, self.nu)\n",
    "        pdf_z = stats.t.pdf(z_p, self.nu)\n",
    "        \n",
    "        es = self.mu + self.sigma * (\n",
    "            pdf_z * (self.nu + z_p**2) / ((1 - p) * (self.nu - 1))\n",
    "        )\n",
    "        return es\n",
    "\n",
    "\n",
    "class FrechetDistribution(HeavyTailedDistribution):\n",
    "    \"\"\"\n",
    "    Fréchet Distribution: F(x) = exp(-x^(-alpha)) for x > 0.\n",
    "    \n",
    "    This is the Type II extreme value distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, mu=0, sigma=1):\n",
    "        super().__init__(alpha)\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def simulate(self, n, random_state=None):\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        u = np.random.uniform(0, 1, n)\n",
    "        return self.mu + self.sigma * (-np.log(u)) ** (-1/self.alpha)\n",
    "    \n",
    "    def pdf(self, x):\n",
    "        z = (x - self.mu) / self.sigma\n",
    "        return np.where(z > 0,\n",
    "                       (self.alpha / self.sigma) * z**(-1-self.alpha) * \n",
    "                       np.exp(-z**(-self.alpha)),\n",
    "                       0)\n",
    "    \n",
    "    def cdf(self, x):\n",
    "        z = (x - self.mu) / self.sigma\n",
    "        return np.where(z > 0, np.exp(-z**(-self.alpha)), 0)\n",
    "    \n",
    "    def var(self, p):\n",
    "        return self.mu + self.sigma * (-np.log(p)) ** (-1/self.alpha)\n",
    "    \n",
    "    def es(self, p):\n",
    "        \"\"\"ES for Fréchet (analytical for alpha > 1).\"\"\"\n",
    "        if self.alpha <= 1:\n",
    "            return np.inf\n",
    "        var_p = self.var(p)\n",
    "        # Using asymptotic relationship\n",
    "        return self.alpha / (self.alpha - 1) * var_p"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create pedagogical examples with different tail indices\n",
    "print(\"=\"*80)\n",
    "print(\"HEAVY-TAILED DISTRIBUTION EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define distributions with pedagogical tail indices\n",
    "distributions = {\n",
    "    'Pareto (α=3)': ParetoDistribution(alpha=3.0, x_m=1.0),\n",
    "    'Student-t (ν=3)': StudentTDistribution(nu=3.0, mu=0, sigma=1),\n",
    "    'Pareto (α=4)': ParetoDistribution(alpha=4.0, x_m=1.0),\n",
    "    'Student-t (ν=4)': StudentTDistribution(nu=4.0, mu=0, sigma=1),\n",
    "    'Fréchet (α=3)': FrechetDistribution(alpha=3.0, mu=0, sigma=1),\n",
    "}\n",
    "\n",
    "# Compare moment existence\n",
    "print(\"\\nMOMENT EXISTENCE (Slide 56):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Distribution':<20} {'α':<8} {'E[X]':<10} {'E[X²]':<10} {'E[X³]':<10} {'E[X⁴]':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, dist in distributions.items():\n",
    "    moments = []\n",
    "    for k in [1, 2, 3, 4]:\n",
    "        if k < dist.alpha:\n",
    "            moments.append(\"Exists\")\n",
    "        else:\n",
    "            moments.append(\"∞\")\n",
    "    print(f\"{name:<20} {dist.alpha:<8.1f} {moments[0]:<10} {moments[1]:<10} {moments[2]:<10} {moments[3]:<10}\")\n",
    "\n",
    "print(\"\\nNote: For α=3, variance exists but skewness doesn't!\")\n",
    "print(\"      For α=4, variance and skewness exist but kurtosis doesn't!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Heavy Tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulate and visualize\n",
    "n_samples = 10000\n",
    "\n",
    "# Focus on Pareto and Student-t with different alphas\n",
    "focus_dists = {\n",
    "    'Pareto (α=2.5)': ParetoDistribution(alpha=2.5, x_m=1.0),\n",
    "    'Pareto (α=3.5)': ParetoDistribution(alpha=3.5, x_m=1.0),\n",
    "    'Student-t (ν=3)': StudentTDistribution(nu=3.0, mu=0, sigma=1),\n",
    "    'Normal': None  # For comparison\n",
    "}\n",
    "\n",
    "samples = {}\n",
    "for name, dist in focus_dists.items():\n",
    "    if dist is not None:\n",
    "        samples[name] = dist.simulate(n_samples, random_state=42)\n",
    "    else:\n",
    "        samples[name] = np.random.standard_normal(n_samples)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. PDF comparison (zoomed to see differences)\n",
    "ax = axes[0, 0]\n",
    "x_range = np.linspace(0, 8, 1000)\n",
    "colors = ['red', 'blue', 'green', 'black']\n",
    "for (name, dist), color in zip(focus_dists.items(), colors):\n",
    "    if dist is not None:\n",
    "        if isinstance(dist, StudentTDistribution):\n",
    "            # Shift and scale for comparison\n",
    "            x_plot = x_range\n",
    "            y = dist.pdf(x_plot)\n",
    "        else:\n",
    "            y = dist.pdf(x_range)\n",
    "            x_plot = x_range\n",
    "        ax.plot(x_plot, y, label=name, lw=2, color=color, alpha=0.8)\n",
    "    else:\n",
    "        ax.plot(x_range, stats.norm.pdf(x_range), label=name, \n",
    "               lw=2, linestyle='--', color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('PDF Comparison\\n(Lower α = Fatter tails)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_xlim([0, 8])\n",
    "ax.set_ylim([0, 0.5])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Log-log tail plot (Slide 45-46)\n",
    "ax = axes[0, 1]\n",
    "for (name, dist), color in zip(focus_dists.items(), colors):\n",
    "    if name != 'Normal':\n",
    "        data = samples[name]\n",
    "        # Create empirical tail: P(X > x)\n",
    "        sorted_data = np.sort(data)\n",
    "        tail_prob = 1 - np.arange(1, len(sorted_data)+1) / len(sorted_data)\n",
    "        \n",
    "        # Focus on right tail\n",
    "        threshold = np.percentile(sorted_data, 90)\n",
    "        mask = sorted_data > threshold\n",
    "        \n",
    "        log_x = np.log(sorted_data[mask])\n",
    "        log_prob = np.log(tail_prob[mask])\n",
    "        \n",
    "        ax.scatter(log_x, log_prob, s=10, alpha=0.5, color=color, label=name)\n",
    "        \n",
    "        # Fit line: log(1-F(x)) ≈ -alpha*log(x) + const\n",
    "        if len(log_x) > 10:\n",
    "            slope, intercept = np.polyfit(log_x, log_prob, 1)\n",
    "            ax.plot(log_x, slope*log_x + intercept, '--', \n",
    "                   color=color, lw=2, alpha=0.7,\n",
    "                   label=f'{name} fit: α≈{-slope:.2f}')\n",
    "\n",
    "ax.set_xlabel('log(x)', fontsize=11)\n",
    "ax.set_ylabel('log(P(X > x))', fontsize=11)\n",
    "ax.set_title('Log-Log Tail Plot\\n(Straight line confirms heavy tail)', \n",
    "            fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Histograms\n",
    "ax = axes[1, 0]\n",
    "for (name, _), color in zip(focus_dists.items(), colors):\n",
    "    # Truncate extreme values for visibility\n",
    "    data_plot = samples[name]\n",
    "    data_plot = data_plot[data_plot < np.percentile(data_plot, 99)]\n",
    "    ax.hist(data_plot, bins=80, alpha=0.5, density=True, \n",
    "           label=name, color=color, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Distribution Comparison\\n(99th percentile truncated)', \n",
    "            fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Q-Q plots against normal\n",
    "ax = axes[1, 1]\n",
    "for (name, _), color in zip(focus_dists.items(), colors):\n",
    "    if name != 'Normal':\n",
    "        stats.probplot(samples[name], dist=\"norm\", plot=ax)\n",
    "        ax.get_lines()[-2].set_color(color)\n",
    "        ax.get_lines()[-2].set_label(name)\n",
    "        ax.get_lines()[-2].set_markersize(3)\n",
    "        ax.get_lines()[-2].set_alpha(0.6)\n",
    "\n",
    "ax.set_title('Q-Q Plot vs Normal\\n(Departure from line = heavy tail)', \n",
    "            fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('heavy_tail_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coherent Risk Measures: VaR vs ES\n",
    "\n",
    "### Coherency Axioms (Slide 3)\n",
    "\n",
    "A risk measure $\\rho$ is **coherent** if it satisfies:\n",
    "\n",
    "1. **Monotonicity**: If $X \\leq Y$, then $\\rho(X) \\leq \\rho(Y)$\n",
    "2. **Translation Invariance**: $\\rho(X + c) = \\rho(X) + c$ for constant $c$\n",
    "3. **Positive Homogeneity**: $\\rho(\\lambda X) = \\lambda \\rho(X)$ for $\\lambda > 0$\n",
    "4. **Subadditivity**: $\\rho(X + Y) \\leq \\rho(X) + \\rho(Y)$\n",
    "\n",
    "**Result (Slides 4-7):**\n",
    "- **VaR**: NOT coherent (fails subadditivity)\n",
    "- **ES**: Coherent (satisfies all axioms)\n",
    "\n",
    "### Counterexample: VaR Subadditivity Failure (Slide 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def demonstrate_var_subadditivity_failure():\n",
    "    \"\"\"\n",
    "    Recreate the bond example from slide 4.\n",
    "    \n",
    "    Two identical bonds:\n",
    "    - Price: 100\n",
    "    - Payoff: 105 with prob (1-p), 0 with prob p\n",
    "    - Loss: -5 with prob (1-p), 100 with prob p\n",
    "    \"\"\"\n",
    "    p = 0.05  # Default probability\n",
    "    \n",
    "    # Choose alpha such that (1-p)^2 < alpha < 1-p\n",
    "    # This ensures: P(L1 > -5) > 1-alpha > P(L1+L2 > -10)\n",
    "    alpha = 0.92  # Between (1-p)^2 = 0.9025 and 1-p = 0.95\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"VaR SUBADDITIVITY FAILURE: Bond Example (Slide 4)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nSetup:\")\n",
    "    print(f\"  Default probability p = {p:.2%}\")\n",
    "    print(f\"  Confidence level α = {alpha:.2%}\")\n",
    "    print(f\"  Condition: (1-p)² = {(1-p)**2:.4f} < α < 1-p = {1-p:.4f}\")\n",
    "    \n",
    "    # Individual bond losses\n",
    "    # L1 = L2: -5 with prob (1-p), 100 with prob p\n",
    "    # P(L > -5) = p = 0.05 < 1-α = 0.08\n",
    "    # Therefore VaR_α(L1) = -5\n",
    "    var_l1 = -5\n",
    "    var_l2 = -5\n",
    "    \n",
    "    # Combined portfolio L = L1 + L2\n",
    "    # L = -10 with prob (1-p)^2 = 0.9025\n",
    "    # L = 95 with prob 2p(1-p) = 0.095\n",
    "    # L = 200 with prob p^2 = 0.0025\n",
    "    \n",
    "    # P(L > 95) = p^2 = 0.0025 < 1-α = 0.08\n",
    "    # P(L > -10) = 2p(1-p) + p^2 = 0.0975 > 1-α = 0.08\n",
    "    # Therefore VaR_α(L) = 95\n",
    "    var_combined = 95\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  VaR_{alpha:.0%}(L₁) = {var_l1}\")\n",
    "    print(f\"  VaR_{alpha:.0%}(L₂) = {var_l2}\")\n",
    "    print(f\"  VaR_{alpha:.0%}(L₁ + L₂) = {var_combined}\")\n",
    "    print(f\"\\nSubadditivity check:\")\n",
    "    print(f\"  VaR(L₁ + L₂) = {var_combined}\")\n",
    "    print(f\"  VaR(L₁) + VaR(L₂) = {var_l1 + var_l2}\")\n",
    "    print(f\"  Violation: {var_combined} > {var_l1 + var_l2}!\")\n",
    "    print(f\"\\n  ⚠️  VaR is NOT SUBADDITIVE!\")\n",
    "    print(f\"      Diversification can INCREASE VaR.\")\n",
    "    \n",
    "    # Monte Carlo verification\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"Monte Carlo Verification\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    n_sim = 100000\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate defaults (independent)\n",
    "    default1 = np.random.binomial(1, p, n_sim)\n",
    "    default2 = np.random.binomial(1, p, n_sim)\n",
    "    \n",
    "    # Calculate losses\n",
    "    L1 = np.where(default1 == 1, 100, -5)\n",
    "    L2 = np.where(default2 == 1, 100, -5)\n",
    "    L_combined = L1 + L2\n",
    "    \n",
    "    # Calculate VaR\n",
    "    var_l1_mc = np.percentile(L1, alpha * 100)\n",
    "    var_l2_mc = np.percentile(L2, alpha * 100)\n",
    "    var_combined_mc = np.percentile(L_combined, alpha * 100)\n",
    "    \n",
    "    print(f\"\\nMonte Carlo estimates (n={n_sim:,}):\")\n",
    "    print(f\"  VaR_{alpha:.0%}(L₁) ≈ {var_l1_mc:.2f}\")\n",
    "    print(f\"  VaR_{alpha:.0%}(L₂) ≈ {var_l2_mc:.2f}\")\n",
    "    print(f\"  VaR_{alpha:.0%}(L₁ + L₂) ≈ {var_combined_mc:.2f}\")\n",
    "    print(f\"  Sum: {var_l1_mc + var_l2_mc:.2f}\")\n",
    "    print(f\"  Violation: {var_combined_mc:.2f} > {var_l1_mc + var_l2_mc:.2f}\")\n",
    "    \n",
    "    return L1, L2, L_combined\n",
    "\n",
    "L1, L2, L_combined = demonstrate_var_subadditivity_failure()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. VaR and ES under Heavy Tails\n",
    "\n",
    "### Probability Shifting (Slide 54)\n",
    "\n",
    "For heavy-tailed distributions with tail index $\\alpha$:\n",
    "\n",
    "$$\\lim_{p\\to 1} \\frac{\\text{VaR}_p}{\\text{VaR}_q} = \\left(\\frac{1-q}{1-p}\\right)^{1/\\alpha}$$\n",
    "\n",
    "### ES-VaR Relationship (Slide 55)\n",
    "\n",
    "For $\\alpha > 1$:\n",
    "$$\\lim_{p\\to 1} \\frac{\\text{ES}_p}{\\text{VaR}_p} = \\frac{\\alpha}{\\alpha - 1}$$\n",
    "\n",
    "**Implications:**\n",
    "- Lower $\\alpha$ → Larger ES/VaR ratio\n",
    "- For $\\alpha = 3$: ES ≈ 1.5 × VaR\n",
    "- For $\\alpha = 4$: ES ≈ 1.33 × VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_probability_shifting(dist, alphas_to_test, q=0.95, sample_size=100000):\n",
    "    \"\"\"\n",
    "    Verify probability shifting relationship (slide 54).\n",
    "    \n",
    "    Tests: VaR_p / VaR_q ≈ ((1-q)/(1-p))^(1/alpha)\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"PROBABILITY SHIFTING ANALYSIS (Slide 54)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nBase confidence level q = {q:.0%}\")\n",
    "    print(f\"Sample size = {sample_size:,}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for alpha in alphas_to_test:\n",
    "        print(f\"\\nTail Index α = {alpha:.1f}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Create distribution\n",
    "        if isinstance(dist, str) and dist == 'pareto':\n",
    "            model = ParetoDistribution(alpha=alpha, x_m=1.0)\n",
    "        elif isinstance(dist, str) and dist == 'student_t':\n",
    "            model = StudentTDistribution(nu=alpha, mu=0, sigma=1)\n",
    "        else:\n",
    "            model = dist\n",
    "        \n",
    "        # Simulate\n",
    "        data = model.simulate(sample_size, random_state=42)\n",
    "        \n",
    "        # Calculate VaR at different levels\n",
    "        var_q = np.percentile(data, q * 100)\n",
    "        \n",
    "        # Test probability shifting for various p > q\n",
    "        test_ps = [0.975, 0.99, 0.995, 0.999]\n",
    "        \n",
    "        print(f\"{'p':<8} {'VaR_p':<12} {'Ratio':<12} {'Theory':<12} {'Error %':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for p in test_ps:\n",
    "            if p > q:\n",
    "                var_p = np.percentile(data, p * 100)\n",
    "                empirical_ratio = var_p / var_q\n",
    "                theoretical_ratio = ((1-q)/(1-p)) ** (1/alpha)\n",
    "                error_pct = (empirical_ratio - theoretical_ratio) / theoretical_ratio * 100\n",
    "                \n",
    "                print(f\"{p:<8.1%} {var_p:<12.4f} {empirical_ratio:<12.4f} \"\n",
    "                     f\"{theoretical_ratio:<12.4f} {error_pct:<12.2f}\")\n",
    "                \n",
    "                results.append({\n",
    "                    'alpha': alpha,\n",
    "                    'p': p,\n",
    "                    'empirical': empirical_ratio,\n",
    "                    'theoretical': theoretical_ratio\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test for different tail indices\n",
    "shifting_results = analyze_probability_shifting('pareto', [2.5, 3.0, 3.5, 4.0], q=0.95)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_es_var_relationship(alphas, sample_size=100000):\n",
    "    \"\"\"\n",
    "    Verify ES/VaR relationship (slide 55).\n",
    "    \n",
    "    Tests: ES_p / VaR_p → alpha/(alpha-1) as p → 1\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ES/VaR RELATIONSHIP ANALYSIS (Slide 55)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Sample size = {sample_size:,}\\n\")\n",
    "    \n",
    "    confidence_levels = [0.90, 0.95, 0.975, 0.99, 0.995, 0.999]\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"\\nTail Index α = {alpha:.1f}\")\n",
    "        print(f\"Theoretical limit: ES/VaR → {alpha/(alpha-1):.4f}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Pareto distribution\n",
    "        dist = ParetoDistribution(alpha=alpha, x_m=1.0)\n",
    "        data = dist.simulate(sample_size, random_state=42)\n",
    "        \n",
    "        print(f\"{'Confidence':<12} {'VaR':<12} {'ES':<12} {'ES/VaR':<12} {'Theory':<12} {'Error %':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for p in confidence_levels:\n",
    "            # Monte Carlo VaR and ES\n",
    "            var_p = np.percentile(data, p * 100)\n",
    "            tail_data = data[data >= var_p]\n",
    "            es_p = tail_data.mean() if len(tail_data) > 0 else var_p\n",
    "            \n",
    "            ratio = es_p / var_p\n",
    "            theory = alpha / (alpha - 1)\n",
    "            error_pct = (ratio - theory) / theory * 100\n",
    "            \n",
    "            print(f\"{p:<12.1%} {var_p:<12.4f} {es_p:<12.4f} {ratio:<12.4f} \"\n",
    "                 f\"{theory:<12.4f} {error_pct:<12.2f}\")\n",
    "            \n",
    "            results_list.append({\n",
    "                'alpha': alpha,\n",
    "                'confidence': p,\n",
    "                'VaR': var_p,\n",
    "                'ES': es_p,\n",
    "                'ratio': ratio,\n",
    "                'theory': theory\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "# Test for typical financial market tail indices\n",
    "es_var_results = analyze_es_var_relationship([2.5, 3.0, 3.5, 4.0], sample_size=100000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize ES/VaR convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: ES/VaR ratio vs confidence level\n",
    "ax = axes[0]\n",
    "for alpha in [2.5, 3.0, 3.5, 4.0]:\n",
    "    data = es_var_results[es_var_results['alpha'] == alpha]\n",
    "    ax.plot(data['confidence'], data['ratio'], 'o-', label=f'α={alpha:.1f}', \n",
    "           lw=2, markersize=6)\n",
    "    \n",
    "    # Theoretical limit\n",
    "    theory = alpha / (alpha - 1)\n",
    "    ax.axhline(theory, linestyle='--', alpha=0.5, lw=1.5,\n",
    "              label=f'α={alpha:.1f} limit: {theory:.3f}')\n",
    "\n",
    "ax.set_xlabel('Confidence Level p', fontsize=11)\n",
    "ax.set_ylabel('ES/VaR Ratio', fontsize=11)\n",
    "ax.set_title('ES/VaR Convergence to Theoretical Limit\\n(Slide 55)',\n",
    "            fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9, ncol=2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Theoretical relationship\n",
    "ax = axes[1]\n",
    "alpha_range = np.linspace(1.5, 6, 100)\n",
    "es_var_limit = alpha_range / (alpha_range - 1)\n",
    "ax.plot(alpha_range, es_var_limit, 'b-', lw=3, label='ES/VaR = α/(α-1)')\n",
    "\n",
    "# Mark typical values\n",
    "for alpha in [2.5, 3.0, 3.5, 4.0]:\n",
    "    limit = alpha / (alpha - 1)\n",
    "    ax.plot(alpha, limit, 'ro', markersize=10)\n",
    "    ax.annotate(f'α={alpha:.1f}\\nES/VaR={limit:.2f}', \n",
    "               xy=(alpha, limit), xytext=(alpha+0.3, limit+0.1),\n",
    "               fontsize=9, ha='left')\n",
    "\n",
    "ax.set_xlabel('Tail Index α', fontsize=11)\n",
    "ax.set_ylabel('Limiting ES/VaR Ratio', fontsize=11)\n",
    "ax.set_title('Theoretical ES/VaR Relationship\\n(Heavy-tailed distributions)',\n",
    "            fontsize=12, fontweight='bold')\n",
    "ax.set_xlim([1.5, 6])\n",
    "ax.set_ylim([1, 3])\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('es_var_relationship.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"=\"*80)\n",
    "print(\"For heavy-tailed distributions (typical α ∈ [3, 4] for finance):\")\n",
    "print(\"  • ES is 25-50% larger than VaR\")\n",
    "print(\"  • Lower α (fatter tails) → larger ES/VaR gap\")\n",
    "print(\"  • Convergence is faster at higher confidence levels (p → 1)\")\n",
    "print(\"  • Emerging markets often have α ∈ [2.5, 3.5] → even larger gap!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basel III Regulatory Change Analysis\n",
    "\n",
    "### Problem Setup (Slide 72, Exercise 2)\n",
    "\n",
    "Basel III changed market risk capital requirement from:\n",
    "- **Basel II**: VaR₉₉%\n",
    "- **Basel III**: ES₉₇.₅%\n",
    "\n",
    "**Question**: How does this affect required capital?\n",
    "\n",
    "Given:\n",
    "- $(1-p)/(1-q) = 2.5$ where $p = 0.99$, $q = 0.975$\n",
    "- Typical tail indices: $\\alpha \\in [3, 4]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def basel_capital_analysis():\n",
    "    \"\"\"\n",
    "    Analyze Basel II → Basel III capital change.\n",
    "    \n",
    "    From slide 72, Exercise 2.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"BASEL II → BASEL III CAPITAL REQUIREMENT ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    p = 0.99   # Basel II\n",
    "    q = 0.975  # Basel III\n",
    "    \n",
    "    print(f\"\\nRegulatory Change:\")\n",
    "    print(f\"  Basel II: VaR₉₉%\")\n",
    "    print(f\"  Basel III: ES₉₇.₅%\")\n",
    "    print(f\"\\nProbability ratio: (1-q)/(1-p) = {(1-q)/(1-p):.2f}\")\n",
    "    \n",
    "    # Test different tail indices\n",
    "    alphas = [2.5, 3.0, 3.5, 4.0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYTICAL RESULTS (Slides 54-55):\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'α':<8} {'VaR_p/VaR_q':<15} {'ES_p/VaR_p':<15} {'ES_p/VaR_q':<15} {'Capital Change':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        # Step 1: VaR_p / VaR_q (probability shifting, slide 54)\n",
    "        var_ratio = ((1-q)/(1-p)) ** (1/alpha)\n",
    "        \n",
    "        # Step 2: ES_p / VaR_p (slide 55)\n",
    "        if alpha > 1:\n",
    "            es_var_ratio = alpha / (alpha - 1)\n",
    "        else:\n",
    "            es_var_ratio = np.inf\n",
    "        \n",
    "        # Step 3: ES_p / VaR_q\n",
    "        total_ratio = es_var_ratio * var_ratio\n",
    "        \n",
    "        # Capital change\n",
    "        capital_change_pct = (total_ratio - 1) * 100\n",
    "        \n",
    "        print(f\"{alpha:<8.1f} {var_ratio:<15.4f} {es_var_ratio:<15.4f} \"\n",
    "             f\"{total_ratio:<15.4f} {capital_change_pct:>+14.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MONTE CARLO VERIFICATION:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    n_sim = 200000\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"\\nα = {alpha:.1f}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Generate data\n",
    "        dist = ParetoDistribution(alpha=alpha, x_m=1.0)\n",
    "        data = dist.simulate(n_sim, random_state=42)\n",
    "        \n",
    "        # Basel II: VaR 99%\n",
    "        var_basel2 = np.percentile(data, p * 100)\n",
    "        \n",
    "        # Basel III: ES 97.5%\n",
    "        var_975 = np.percentile(data, q * 100)\n",
    "        tail_data = data[data >= var_975]\n",
    "        es_basel3 = tail_data.mean()\n",
    "        \n",
    "        ratio_mc = es_basel3 / var_basel2\n",
    "        change_mc = (ratio_mc - 1) * 100\n",
    "        \n",
    "        print(f\"  VaR₉₉% (Basel II):   {var_basel2:.4f}\")\n",
    "        print(f\"  ES₉₇.₅% (Basel III):  {es_basel3:.4f}\")\n",
    "        print(f\"  Ratio:               {ratio_mc:.4f}\")\n",
    "        print(f\"  Capital change:      {change_mc:+.2f}%\")\n",
    "    \n",
    "    return\n",
    "\n",
    "basel_capital_analysis()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULATORY IMPLICATIONS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. Banks must hold 6-13% MORE capital under Basel III\")\n",
    "print(\"   (depending on tail heaviness)\")\n",
    "print(\"\\n2. Heavier tails (lower α) → larger capital increase\")\n",
    "print(\"   - Emerging markets (α ≈ 2.5-3): ~10-13% increase\")\n",
    "print(\"   - Developed markets (α ≈ 3.5-4): ~6-8% increase\")\n",
    "print(\"\\n3. This addresses VaR's coherence failure\")\n",
    "print(\"   - ES accounts for tail losses beyond VaR\")\n",
    "print(\"   - No 'diversification paradox'\")\n",
    "print(\"\\n4. Tradeoff: More conservative but harder to backtest\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Key Takeaways\n",
    "\n",
    "### Heavy-Tailed Distributions\n",
    "- **Definition**: $\\lim_{t\\to\\infty} \\frac{1-F(tx)}{1-F(t)} = x^{-\\alpha}$\n",
    "- **Tail index** $\\alpha$ determines:\n",
    "  - Moment existence: $E[X^k]$ finite only for $k < \\alpha$\n",
    "  - Tail decay rate: lower $\\alpha$ = slower decay = fatter tails\n",
    "- **Financial applications**: Typical $\\alpha \\in [2.5, 4]$\n",
    "\n",
    "### VaR vs ES\n",
    "- **VaR**: Simple quantile, NOT coherent (fails subadditivity)\n",
    "  - Can discourage diversification!\n",
    "  - Ignores tail shape beyond threshold\n",
    "  \n",
    "- **ES**: Coherent, accounts for tail severity\n",
    "  - Harder to estimate and backtest\n",
    "  - Better for extreme risk management\n",
    "\n",
    "### Probability Shifting\n",
    "$$\\text{VaR}_p \\approx \\text{VaR}_q \\left(\\frac{1-q}{1-p}\\right)^{1/\\alpha}$$\n",
    "\n",
    "### ES-VaR Relationship\n",
    "$$\\text{ES}_p \\approx \\frac{\\alpha}{\\alpha-1} \\text{VaR}_p$$\n",
    "\n",
    "### Basel III Implications\n",
    "- ES₉₇.₅% requires 6-13% more capital than VaR₉₉%\n",
    "- Change magnitude depends on tail index\n",
    "- Addresses coherence concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercises\n",
    "\n",
    "### Exercise 1: Custom Heavy-Tailed Distribution\n",
    "Create a mixture of Pareto distributions with different tail indices. How does this affect:\n",
    "- The effective tail index?\n",
    "- ES/VaR ratio?\n",
    "- Probability shifting accuracy?\n",
    "\n",
    "### Exercise 2: Emerging Market Application\n",
    "Use emerging market equity data to:\n",
    "1. Estimate tail index using Hill estimator (see next notebook)\n",
    "2. Calculate VaR₉₉% and ES₉₇.₅%\n",
    "3. Compare with Basel regulatory capital\n",
    "\n",
    "### Exercise 3: Coherence Verification\n",
    "Implement numerical tests for all four coherence axioms for:\n",
    "- VaR\n",
    "- ES\n",
    "- Standard deviation\n",
    "\n",
    "### Exercise 4: Sensitivity Analysis\n",
    "How sensitive are the Basel capital calculations to:\n",
    "- Tail index estimation error?\n",
    "- Sample size?\n",
    "- Extreme observations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
